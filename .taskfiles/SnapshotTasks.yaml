---
version: "3"

vars:
  NAMESPACE: '{{ .NAMESPACE | default "default" }}'
  KOPIA_NAMESPACE: '{{ .KOPIA_NAMESPACE | default "default" }}'
  TS: '{{ now | date "20060102150405" }}'
  TIMEOUT: '{{ .TIMEOUT | default "1m" }}'

x-preconditions:
  - &has-cronjob
    sh: kubectl get cronjob -n {{.NAMESPACE}} {{.PVC}}-snapshot
    msg: "CronJob '{{.PVC}}-snapshot' in namespace '{{.NAMESPACE}}' not found"
  - &has-pvc
    sh: kubectl get pvc -n {{.NAMESPACE}} {{.PVC}}
    msg: "PersistentVolumeClaim '{{.PVC}}' in namespace '{{.NAMESPACE}}' not found"
  - &has-restore-job
    msg: "File '{{.RESTORE_JOB_FILENAME}}' not found"
    sh: "test -f {{.RESTORE_JOB_FILENAME}}"
  - &has-kopia-deployment
    msg: "deployment/kopia in namespace/{{.KOPIA_NAMESPACE }} not found"
    sh: kubectl -n {{.KOPIA_NAMESPACE }} get deployment kopia

tasks:
  list:
    desc: List all existing snapshots for an app (task snapshot:list PVC=plex-config [NAMESPACE=<namespace> KOPIA_NAMESPACE=<namespace>])
    # vars: *vars
    preconditions:
      - *has-kopia-deployment
    cmds:
      - kubectl -n {{.KOPIA_NAMESPACE }} exec -it deployment/kopia -- kopia snapshot list /data/{{.NAMESPACE}}/{{.PVC}} {{.CLI_ARGS}}

  create:
    desc: Create a job to snapshot a PVC (ex. task snapshot:create PVC=plex-config [NAMESPACE=<namespace>])
    vars:
      SNAPSHOT_JOB_NAME: "{{.PVC}}-snapshot-{{.TS}}"
      SNAPSHOT_JOB_SELECTOR: "job-name={{.SNAPSHOT_JOB_NAME}}"
    preconditions:
      - *has-cronjob
      - *has-pvc
    cmds:
      - defer: kubectl -n {{.NAMESPACE}} delete job {{.SNAPSHOT_JOB_NAME}}
      - |
        kubectl -n {{.NAMESPACE}} create job --from=cronjob/{{.PVC}}-snapshot {{.SNAPSHOT_JOB_NAME}} --dry-run=client --output yaml \
          | yq eval "del(.spec.template.spec.initContainers)" - \
          | kubectl apply -f -
      - sleep 2
      - kubectl -n {{.NAMESPACE}} wait pod --for condition=ready --selector={{.SNAPSHOT_JOB_SELECTOR}} --timeout={{.TIMEOUT}}
      - kubectl -n {{.NAMESPACE}} logs -f job/{{.SNAPSHOT_JOB_NAME}} -c backup

  create_all:
    desc: Run `task snapshot:create` for all PVC's that have snapshots enabled
    cmds:
      - |
        kubectl get pvc --all-namespaces --no-headers=true --selector="snapshot.home.arpa/enabled=true" \
          | awk "{print \$1,\$2}" \
          | xargs -L1 sh -c 'task snapshot:create NAMESPACE=$0 PVC=$1'

  restore:
    desc: Spawn a job to restore an app from a snapshot (task snapshot:restore APP=plex [SNAPSHOT=(latest|<snapshot-id>) NAMESPACE=<namespace> KOPIA_NAMESPACE=<namespace> TIMEOUT=<timeout>])
    vars:
      SNAPSHOT: '{{.SNAPSHOT | default "latest"}}'
      RESTORE_JOB_FILENAME: "{{.PROJECT_DIR}}/hack/restore-job.yaml"
      RESTORE_JOB_NAME: "{{.PVC}}-snapshot-restore-{{.TS}}"
      RESTORE_JOB_SELECTOR: "job-name={{.RESTORE_JOB_NAME}}"
      APP_SELECTOR: "app.kubernetes.io/name={{.APP}}"
      NAME:
        sh: kubectl get deploy,sts -n {{.NAMESPACE}} --selector="{{.APP_SELECTOR}}" --no-headers | awk '{print $1}'
    env:
      NAS_ADDRESS: "{{.NAS_ADDR}}"
      NAMESPACE: "{{.NAMESPACE}}"
      PVC: "{{.PVC}}"
      RESTORE_JOB_NAME: "{{.RESTORE_JOB_NAME}}"
      SNAPSHOT:
        sh: |
          if [[ {{.SNAPSHOT}} == "latest" ]]; then
            kubectl exec deployment/kopia -n {{.KOPIA_NAMESPACE }} -- kopia snapshot list /data/{{.NAMESPACE}}/{{.PVC}} --json | jq --raw-output '.[-1] | .id'
          else
            echo {{.SNAPSHOT}}
          fi
    preconditions:
      - *has-restore-job
    cmds:
      - defer: flux -n {{.NAMESPACE}} resume helmrelease {{.APP}}
      - defer: kubectl -n {{.NAMESPACE}} delete job {{.RESTORE_JOB_NAME}}
      - flux -n {{.NAMESPACE}} suspend helmrelease {{.APP}}
      - kubectl -n {{.NAMESPACE}} scale {{.NAME}} --replicas 0
      - kubectl -n {{.NAMESPACE}} wait pod --for delete --selector="{{.APP_SELECTOR}}" --timeout=2m
      - envsubst < <(cat {{.RESTORE_JOB_FILENAME}}) | kubectl apply -f -
      - sleep 2
      - kubectl -n {{.NAMESPACE}} wait job/{{.RESTORE_JOB_NAME}} --for condition=complete --timeout={{.TIMEOUT}}
      - kubectl -n {{.NAMESPACE}} logs job/{{.RESTORE_JOB_NAME}} -c restore
